{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from my_paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLS_PATH = xls_path\n",
    "DATA_DIR = data_dir\n",
    "OUTPUT_DIR = output_dir\n",
    "PICKLE_DIR = pickle_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes before: 20\n",
      "num classes after:  14\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(XLS_PATH)\n",
    "df['STE'] = df['STE'].str.replace(\"\\t\",\" \")\n",
    "df['STE'] = df['STE'].str.strip() \n",
    "df['len'] = df['STE'].apply(lambda x: len(x.split()))\n",
    "df = df[df.len >= 10]\n",
    "\n",
    "# get categories with at least 10 counts\n",
    "counts = df.groupby('TEXT_TYPE').count()['TIMID']\n",
    "cols = counts[counts >= 10].index\n",
    "\n",
    "print('num classes before: {}\\nnum classes after:  {}'\n",
    "      .format(len(df.TEXT_TYPE.unique()), len(cols)))\n",
    "\n",
    "df = df[df.TEXT_TYPE.isin(cols)] # drop labels with too few samples\n",
    "assert(df.TEXT_TYPE.isna().sum() == 0) # make sure there are no NaN in target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.STE)\n",
    "y = np.array(df.TEXT_TYPE)\n",
    "\n",
    "# shuffle data\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "with open(DATA_DIR+'/train.tsv', 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t', quotechar='\\\"')\n",
    "        for lab, text in zip(y_train, X_train):\n",
    "            writer.writerow([lab, text])\n",
    "            \n",
    "with open(DATA_DIR+'/dev.tsv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t', quotechar='\\\"')\n",
    "    for lab, text in zip(y_test, X_test):\n",
    "        writer.writerow([lab, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\\r\\n'\n",
      "FINISHED CV ITERATION: 1\n",
      "b'Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\\r\\n'\n",
      "FINISHED CV ITERATION: 2\n",
      "b'Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\\r\\n'\n",
      "FINISHED CV ITERATION: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import subprocess\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "i = 1\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    with open(DATA_DIR+'/train.tsv', 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t', quotechar='\\\"')\n",
    "        for lab, text in zip(y_train, X_train):\n",
    "            writer.writerow([lab, text])\n",
    "            \n",
    "    with open(DATA_DIR+'/dev.tsv', 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t', quotechar='\\\"')\n",
    "        for lab, text in zip(y_test, X_test):\n",
    "            writer.writerow([lab, text])\n",
    "            \n",
    "    bash_cmd = f\"\"\"python run_classifier.py\n",
    "    --data_dir={DATA_DIR}\n",
    "    --bert_model=bert-base-uncased\n",
    "    --task_name=ste\n",
    "    --output_dir={OUTPUT_DIR}{i}\n",
    "    --gradient_accumulation_steps=2\n",
    "    --do_lower_case\n",
    "    --train_batch_size=32\n",
    "    --learning_rate=2e-5\n",
    "    --num_train_epochs=3\n",
    "    --max_seq_length=128\n",
    "    --do_train\n",
    "    --do_eval\"\"\"\n",
    "\n",
    "    process = subprocess.Popen(bash_cmd.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "    print(output)\n",
    "    print(f'FINISHED CV ITERATION: {i}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for i in range(1,6):\n",
    "    with open(f'{OUTPUT_DIR}{i}/eval_results.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if 'eval_accuracy' in line:\n",
    "                lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(float(x.replace('\\n', '').split()[-1]) for x in lines) / len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [float(x.replace('\\n', '').split()[-1]) for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(accs) / len(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(accs, open(PICKLE_DIR+'/BERT-cv-50-epochs-128-max-seq-len.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
